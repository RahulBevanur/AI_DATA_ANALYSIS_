{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Data Preprocessing\n",
    "\n",
    "#### Always Explore & Visualize Data First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame\n",
    "df.iloc[0:100, 0] = np.nan  \n",
    "X = df.drop(columns='MedHouseVal')\n",
    "y = df['MedHouseVal']\n",
    "missing_values = X.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values)\n",
    "print(\"\\nSummary statistics (to detect outliers):\\n\", X.describe())\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing & Inconsistent Data Before Applying ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame\n",
    "df.iloc[0:100, 0] = np.nan \n",
    "X = df.drop(columns='MedHouseVal')\n",
    "y = df['MedHouseVal']\n",
    "X_dropped = X.dropna()\n",
    "y_dropped = y[X_dropped.index] \n",
    "print(\"Shape after dropping missing values:\", X_dropped.shape) \n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_filled = imputer.fit_transform(X)\n",
    "print(\"Shape after imputation:\", X_filled.shape)\n",
    "\n",
    "def cap_outliers(df, lower_percentile=0.01, upper_percentile=0.99):\n",
    "    capped_df = df.copy()\n",
    "    for col in df.columns:\n",
    "        lower = df[col].quantile(lower_percentile)\n",
    "        upper = df[col].quantile(upper_percentile)\n",
    "        capped_df[col] = np.clip(df[col], lower, upper)\n",
    "    return capped_df\n",
    "X_capped = cap_outliers(pd.DataFrame(X_filled, columns=X.columns))\n",
    "print(\"Outliers handled via capping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the Right Scaling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "data = {'feature1': [10, 20, 30, 40, 50],\n",
    "        'feature2': [1, 2, 3, 4, 5]}\n",
    "df = pd.DataFrame(data)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "print(scaled_df)\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "data = {'feature1': [10, 20, 30, 40, 500],  \n",
    "        'feature2': [1, 2, 3, 4, 5]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "print(scaled_df)\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "data = {'feature1': [-30, -20, 0, 20, 30],\n",
    "        'feature2': [1, 2, 3, 4, 5]}\n",
    "df = pd.DataFrame(data)\n",
    "scaler = MaxAbsScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "print(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Track of Data Transformations for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "import pandas as pd\n",
    "logging.basicConfig(filename='data_preprocessing.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "data = {'feature1': [10, 20, 30, 40, 50],\n",
    "        'feature2': [1, 2, 3, 4, 5]}\n",
    "df = pd.DataFrame(data)\n",
    "def log_preprocessing_step(step_name, details):\n",
    "    logging.info(f\"Step: {step_name}, Details: {details}\")\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_robust = RobustScaler()\n",
    "scaler_maxabs = MaxAbsScaler()\n",
    "scaler_minmax.fit(df)\n",
    "scaled_data_minmax = scaler_minmax.transform(df)\n",
    "log_preprocessing_step(\"Min-Max Scaling\", {\"min\": scaler_minmax.data_min_, \"max\": scaler_minmax.data_max_})\n",
    "scaler_robust.fit(df)\n",
    "scaled_data_robust = scaler_robust.transform(df)\n",
    "log_preprocessing_step(\"Robust Scaling\", {\"median\": scaler_robust.center_, \"IQR\": scaler_robust.scale_})\n",
    "scaler_maxabs.fit(df)\n",
    "scaled_data_maxabs = scaler_maxabs.transform(df)\n",
    "log_preprocessing_step(\"MaxAbs Scaling\", {\"max_abs\": scaler_maxabs.max_abs_})\n",
    "print(scaled_data_minmax)\n",
    "print(scaled_data_robust)\n",
    "print(scaled_data_maxabs)\n",
    "\n",
    "#.\n",
    "import pickle\n",
    "def store_transformation_parameters(scaler, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "store_transformation_parameters(scaler_minmax, 'minmax_scaler.pkl')\n",
    "store_transformation_parameters(scaler_robust, 'robust_scaler.pkl')\n",
    "store_transformation_parameters(scaler_maxabs, 'maxabs_scaler.pkl')\n",
    "def load_transformation_parameters(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "    return scaler\n",
    "loaded_scaler_minmax = load_transformation_parameters('minmax_scaler.pkl')\n",
    "print(loaded_scaler_minmax.data_min_, loaded_scaler_minmax.data_max_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
